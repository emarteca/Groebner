
\documentclass[letterpaper,12pt,titlepage,oneside,final]{book}
 

\newcommand{\package}[1]{\textbf{#1}} 
\newcommand{\cmmd}[1]{\textbackslash\texttt{#1}} 
\newcommand{\href}[1]{#1} 

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}

\algnewcommand\algorithmicInput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicInput]}

\algnewcommand\algorithmicOutput{\textbf{Output:}}
\algnewcommand\Output{\item[\algorithmicOutput]}

\newcommand\abs[1]{\left|#1\right|}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}


\usepackage{ifthen}
\usepackage{indentfirst}
\newboolean{PrintVersion}
\setboolean{PrintVersion}{false} 

\usepackage{amsmath,amssymb,amstext}
\usepackage[pdftex]{graphicx}

\newcommand\Tstrut{\rule{0pt}{2.6ex}}
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}
\newcommand{\TBstrut}{\Tstrut\Bstrut}

\usepackage{lipsum}
\newcounter{examplecounter}
\newenvironment{example}{\begin{quote}%
    \refstepcounter{examplecounter}%
  \textbf{Example \arabic{examplecounter}}%
  \quad
}{%
\end{quote}%
}

\newcounter{definitioncounter}
\newenvironment{defn}{\begin{quote}%
    \refstepcounter{definitioncounter}%
  \textbf{Definition \arabic{definitioncounter}}%
  \quad
}{%
\end{quote}%
}

\newcounter{notecounter}
\newenvironment{note}{\begin{quote}%
    \refstepcounter{notecounter}%
  \textbf{Note \arabic{notecounter}}%
  \quad
}{%
\end{quote}%
}

\newcounter{theoremcounter}
\newenvironment{theorem}{\begin{quote}%
    \refstepcounter{theoremcounter}%
  \textbf{Theorem \arabic{theoremcounter}}%
  \quad
}{%
\end{quote}%
}

\newcounter{Table}[section]

\usepackage{multirow}

\usepackage[pdftex,letterpaper=true,pagebackref=false]{hyperref} 

\usepackage[toc,page]{appendix}
    
\hypersetup{
    plainpages=false,       % needed if Roman numbers in frontpages
    pdfpagelabels=true,     % adds page number as label in Acrobat's page count
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={uWaterloo\ LaTeX\ Thesis\ Template},    % title: CHANGE THIS TEXT!
%    pdfauthor={Author},    % author: CHANGE THIS TEXT! and uncomment this line
%    pdfsubject={Subject},  % subject: CHANGE THIS TEXT! and uncomment this line
%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords, and uncomment this line if desired
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=blue,         % color of internal links
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}
\ifthenelse{\boolean{PrintVersion}}{   
\hypersetup{  % override some previously defined hyperref options
%    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black}
}{} % end of ifthenelse (no else)


\setlength{\marginparwidth}{0pt} 
\setlength{\marginparsep}{0pt}
\setlength{\evensidemargin}{0.125in}
\setlength{\oddsidemargin}{0.125in}
\setlength{\textwidth}{6.375in} 
\raggedbottom


\setlength{\parskip}{\medskipamount}


\renewcommand{\baselinestretch}{1} 

\let\origdoublepage\cleardoublepage
\newcommand{\clearemptydoublepage}{%
  \clearpage{\pagestyle{empty}\origdoublepage}}
\let\cleardoublepage\clearemptydoublepage

%======================================================================
%   L O G I C A L    D O C U M E N T -- the content of your thesis
%======================================================================
\begin{document}


%======================================================================
\chapter{Introduction}
%======================================================================

%----------------------------------------------------------------------
\section{Brief Overview and History of Groebner Bases}
%----------------------------------------------------------------------
 
The problem of solving for a Groebner basis was resolved by Bruno Buchberger in <YEAR>.  His algorithm (\textbf{Buchberger's Algorithm}, explained later in the text) was published in <SOME GARBAGE CONFERENCE PAPER> and it was not until <YEAR> at <SOME CONFERENCE> that the community realized that the algorithm existed and the implications of such.

Buchberger's algorithm was the initial approach, and since then there have been various different solutions 

USES FOR GROEBNER

\section{Intermediate Expression Swell}

Expression swell is a problem emergent in computer algebra.

Working with exact computations, where the numerical precision used is not bounded by the language, provides a platform for situations where as computations proceed the values stored keep increasing in size.  Consider multiplication of integers as an example.  Multiplying two n-digit integers results in a 2n-digit integer.  In computations involving sequential multiplication of integers, the product would get exponentially large.  Another example of expression swell can be seen in rational addition. Consider the following demonstrative example:

\begin{equation*}
  \frac{100}{101} + \frac{101}{102} = \frac{20401}{10302}
\end{equation*}

Here, the initial numerators and denominators are all 3-digit numbers, however their sum is a rational number with the numerator and denominator both 5 digits.  Addition of rational numbers results in expression swell as the greatest common denominator increases as more values are included in the sum.

This is not a problem in classic numerical programming languages where the precision of values stored is bounded by the language specifications.  However, when using a symbolic language where computations are exact, the precision is arbitrary and depends directly on the values stored.  Because of this essentially limitless precision, the results are accurate, however expression swell could potentially result in a large amount of memory being required to store values.

Intermediate expression swell is the special case of expression swell where the intermediate results of a computation have increasingly large values (i.e. suffer from expression swell), however this is not reflected in the final result.  This is not as simple to detect as regular expression swell; to be sure some or all of the intermediate computations must be displayed. 

Groebner basis computations are known to suffer from intermediate expression swell.  Generally the output basis does not reflect the size of the coefficients of the intermediate polynomials.  The intermediate coefficient growth is related to many factors of the original basis: degree, number of polynomials, initial coefficient size.  The values can grow to impractical size, slowing down the execution of the computation and perhaps even halting it if the program runs out of memory for storage.  

Consider the following demonstrative example, as provided by Arnolds:

\begin{eqnarray*}
  f_1 &=& 8x^2y^2 + 5xy^3 + 3x^3z + x^2yz\\
  f_2 &=& x^5 + 2y^3z^2 + 13y^2z^3 + 5yz^4\\ 
  f_3 &=& 8x^3 + 12y^3 + xz^2 + 3\\
  f_4 &=& 7x^2y^4 + 18xy^3z^2 + y^3z^3
\end{eqnarray*} 

As the input basis, while the output basis is computed as:

\begin{eqnarray*}
  g_1 &=& x\\
  g_2 &=& y^3 + \frac{1}{4}\\ 
  g_3 &=& z^2\\
\end{eqnarray*} 

The coefficients and for these polynomials are all less than 10; the resulting basis for ${B = [f_1, f_2, f_3, f_4]}$, with degree based monomial ordering\footnote{Referred to in her paper as DegRevLex, degree reverse lexicographical ordering; however this is equivalent to degree based ordering tdeg in Maple} ${x > y > z}$, given by ${G = [g_1, g_2, g_3]}$ also has small degree and coefficients.  However, the intermediate computations contain polynomials with very large coefficients, rational numbers commonly of 80 000 digits in both the numerator and denominator (CITE ARNOLDS HERE).  

This is a common occurrence in Groebner basis computations.  The size of the intermediate coefficients is not generally reflected in the final output of the normalized basis; it also depends on many factors (degree, initial coefficient size, initial basis size, initial ideal, monomial ordering chosen) and as such is hard to avoid.  Instead of attempting the difficult task of working around it, another solution to this problem will be attempted.



\chapter{Background}

There is some background mathematics necessary to the computation and discussion of Groebner bases.  This will build off assumed knowledge of basic groups and rings in order to explain the ideal theory needed.

\section{Algebraic Review}

There are various algebraic objects which one must be familiar with; this will be a brief introduction to these objects, assuming previous knowledge of the theory of groups and rings.  

There are various algebraic spaces with characterizations between rings and fields - notably, integral domains, unique factorization domains (henceforth UFDs), and Euclidean domains (henceforth EuDs).  These have their own properties, and once the space is extended (for example, by a variable or set of variables in the definition of a polynomial space) they have different relations between each other.  

Each will first be defined, building off the definition of a ring which is presumed known (but included for reference).

\begin{defn}\label{defn: Ring}
  A \textbf{ring} is a triple ${(R, +, *)}$ consisting of a non-empty set ${R}$ along with a pair of binary operations 

  \begin{equation*}
    + : R \times R \rightarrow R   \qquad \qquad * : R \times R \rightarrow R
  \end{equation*}

  such that:

  \begin{itemize}
    \item ${(R, +)}$ is an Abelian group
    \item ${*}$ is associative
    \item ${*}$ is distributive over ${+}$ i.e. ${\forall\, a, b, c\, \in\, R}$\begin{eqnarray*} a*(b+c) = (a*b) + (a*c) \\ (a+b)*c = (a*c) + (b*c) \end{eqnarray*} 
  \end{itemize}
\end{defn}

Rings can also be \textbf{commutative rings}, where the operation ${*}$ is also commutative (recall, ${+}$ is already commutative since ${(R, +)}$ is an Abelian group).  In addition, rings can be \textbf{rings with unity}; this is where ${R}$ has an identity with respect to ${*}$.  In order to construct the next algebraic structures, rings with unity will mainly be considered.

Similar to a ring, but with more specifications, there are integral domains.  

\begin{defn}\label{defn: Integral domain}
  An \textbf{integral domain} is a commutative ring with unity which satisfies the "cancellation law" i.e. \\
  \begin{equation*} \forall\, a, b, c\, \in R\, \mid \, a \cdot b = a \cdot c, \, a \neq 0\end{equation*}
  we must have ${b = c}$

  Note: this is equivalent to the "no zero divisors" law: whenever ${a \cdot b = 0}$, then either ${a = 0}$ or ${b = 0}$
\end{defn}

All integral domains are rings; they just have additional conditions which must be met in order to be categorized as such.

Similarly, there are also UFDs, which are integral domains with more restrictions.  These require the concepts of \textbf{units}, \textbf{associates}, and \textbf{primes}.  These definitions are not explicitly stated in the body of the text as they are more background, however they are included for reference in the Supplementary Definitions appendix \ref{appendix:definitions}.

\begin{defn}\label{defn: Unique Factorization Domain}
  An integral domain ${D}$ is a \textbf{unique factorization domain} (UFD) if for any ${a \in D - \{0\}}$,  either: 
  \begin{itemize}
    \item ${a}$ is a \textbf{unit}, or
    \item ${a = p_1 \cdot p_2 \cdot \ldots \cdot p_n}$ for primes ${p_i}$, and this factorization is unique up to \textbf{associates} and \textbf{reordering}
  \end{itemize}
\end{defn}

Another algebraic structure, building on UFDs, is a \textbf{Euclidean domain}.

\begin{defn}\label{defn: Euclidean domain}
  A \textbf{Euclidean domain} (EuD) is a UFD with a \\ \textbf{valuation function} ${v : D - \{0\} \leftarrow \mathbb{N}_0}$:
  \begin{itemize}
    \item ${v(a, b) \geq v(b) \;\; \forall \, a, b \in D - \{0\}}$
    \item ${\forall \, a, b \in D, b \neq 0, \exists \, q, r \in D \mid a = b \cdot q + r}$ where either ${r = 0}$ or ${v(r) < v(b)}$
  \end{itemize}
\end{defn}

Finally, there is the concept of a \textbf{field}, again building on the previous algebraic structures. 

\begin{defn}\label{defn: Field}
  A \textbf{field} is a commutative ring with unity ${(F, +, *)}$ in which:
  \begin{itemize}
    \item ${(F, +)}$ is an Abelian group
    \item ${(F - \{0\}, *)}$ is an Abelian group (i.e. every nonzero element in ${F}$ has an inverse with respect to ${*}$)
    \item ${*}$ is distributive over ${+}$
  \end{itemize}
\end{defn}

\begin{note}
  Every field is a EuD.

  \textbf{Proof}: For a space to be a EuD, it must have a valuation function ${v : F - \{0\} \leftarrow \mathbb{N}_0}$ satisfying the criteria listed above.
  Here, take the valuation function to be ${v(a) = 1 \, \forall \, a \, \in F - \{0\}}$, for ${F}$ field with unity 1.    
  \begin{itemize}
    \item ${v(a * b) = 1 \, \forall \, a, b \, \in F - \{0\}}$, since ${a * b \in F - \{0\} \forall \, a, b \, \in F - \{0\}}$, and ${v(a) = 1 \, \forall \, a \, \in F - \{0\}}$ \\ This satisfies the first condition of EuD, since ${v(a * b) = 1 \leq 1 = v(b) \, \forall \, a, b \, \in F - \{0\}}$
    \item ${a, b \, \in F, b \neq 0}$ \begin{equation*} a = b * q + r; \; q, r \, \in F, v(r) < v(b) \, \text{or} \, r = 0 \end{equation*} \\ Here, consider that ${v(a) = 1 \, \forall \, a \in F - \{0\}}$.  So, there is no ${r \in F - \{0\}}$ such that ${v(r) < v(b)}$ for some ${b \in F - \{0\}}$.  \\ So therefore, ${r = 0}$ \\ Then, ${a = b*q}$ for some ${q \in F - \{0\}}$ \\ Recall that in a field F, ${b^{-1} \exists \, \forall \, b \in F - \{0\}}$ \\ So, applying ${b^{-1}}$ to both sides, ${b^{-1} * a = q \; \in F \, \forall a, b \in F, b \neq 0}$ \\ This satisfies the second condition of EuD. 
  \end{itemize} 

  Therefore, every field is an EuD.
\end{note}


\section{Polynomials and Polynomial Ideals}

Polynomials are some of the most important and widely used algebraic objects in symbolic computation (CN).  Polynomial are defined as elements of an extension of a space S by one (or more, for multivariate) variables.  The polynomial spaces of interest are generally those where the space extended was a field i.e. where the coefficients are all elements in the field - in these cases the coefficients will have inverses with respect to both ring additive and multiplicative operations.  

Polynomials can be classified by their number of variables - when considering polynomial spaces, they are mainly divided into univariate and multivariate (greater than one variable) spaces.  The distinction between univariate and multivariates (and the conglomeration of all multivariates in this categorization) is due to the behaviour of algebraic spaces when extended by variables (i.e. the space may become another type of space, "lower" in the categorization of algebraic spaces, upon extension), as described below.

First, consider spaces consisting of univariate polynomials; this concept will later be extended to multivariate polynomials.

\begin{defn}\label{defn: Univariate Polynomial Space}
  Given commutative ring with unity ${(R, +, *)}$; write ${R[x]}$ to denote the set of all expressions of the form
  \begin{equation*}
    a(x) = \sum_{k=0}^{\infty} a_k x^k
  \end{equation*}
  where ${a_k \in R}$ (coefficients of ${a(x)}$) 
\end{defn}

The categorization of the initial space determines what type the univariate polynomial space will be.  The relations between these spaces when they are extended is shown in the below list, and in FIGURE 1.
\begin{itemize}
  \item ${D[x]}$ is a \textbf{commutative ring} for ${D}$ \textbf{commutative ring} 
  \item ${D[x]}$ is an \textbf{integral domain} for ${D}$ \textbf{integral domain} 
  \item ${D[x]}$ is a \textbf{UFD} for ${D}$ \textbf{UFD}
  \item ${D[x]}$ is a \textbf{UFD} for ${D}$ \textbf{EuD}
  \item ${D[x]}$ is a \textbf{EuD} for ${D}$ \textbf{field}; with valuation function ${v(p(x)) = deg(p(x))}$ 
\end{itemize}  

This leads to a few interesting points: for one thing, there will never be a polynomial field.  A polynomial extension over a field results in a EuD, and not a field.  This makes sense, as for a space to be a field every non-zero element in the set would need to have an inverse also in the set - and polynomials do not have inverses in the polynomial space (i.e. there are no polynomials ${p_1(x) \neq 1, p_2(x) \neq 1}$ such that ${p_1(x) \cdot p_2(x) = 1 \, \forall \, x}$).  So, a polynomial space cannot be a field.

Another point to consider is the extension of the concept to multivariate polynomials.  To approach this problem, multivariate polynomials can be represented as univariate polynomials extended recursively by the other variables.  This way, the same definitions and group relationships can be used in figuring out what type of space will be present.  

For example, consider a bivariate polynomial space over a ring.  This is equivalent to a univariate space over a univariate space over a ring.  A univariate space over a ring is a ring; then, again, a univariate space over a ring is a ring.  So, a bivariate space over a ring is a ring.

As a more involved example, consider a bivariate polynomial over a field.  The first "layer" to consider is a univariate extension of a field, which is a EuD.  Then, when this space is extended with another variable, this is now a EuD extended by a variable, which results in a UFD.  

\begin{note}
  There cannot be a univariate polynomial field (the "highest" algebraic structure possible for a univariate space is when the coefficient space is a field, and then the extension becomes a EuD).
  
  Similarly, there cannot be a bivariate (or greater than 2 variable) polynomial EuD (or field) - the "highest" algebraic structure is a UFD.
\end{note}      

A polynomial space has a basis which consists of polynomials in the space; like a basis for any other space, this is a set of linearly independent set elements which span the space.  The space can then be written as the set generated by the basis set of polynomials.  For example notation, consider:
EXAMPLE FROM NOTES

Polynomial ideals are essential to the groundwork necessary for Groebner bases; however, first the concept of ideals must be introduced.

\subsection{Ideals}

Ideals are another algebraic structure, building off of rings and ring structure.  

\begin{defn}\label{defn: Ideals}
  An additive subgroup N of ring ${(R, +, *)}$ is an \textbf{ideal} iff 
  \begin{equation*} aN \subseteq R \qquad ; \qquad Nb \subseteq R \; \forall a, b \in R \end{equation*}
  This criterion can be written equivalently as
  \begin{equation*} a*n, \, n*a \in N \, \forall \, n \in N, \, a \in R \end{equation*}
  
  In other words: ideals are groups over a ring which are closed with respect to the ring's multiplication operation i.e. every ideal N contains all of its cosets (and both left and right).  
\end{defn}

The idea of an ideal can be demonstrated in part by the following simple example.  

\begin{example}\label{ex: Simple ideal}
  One classic demonstrative example of an ideal is the even numbers over the integers.\\ 
  The even numbers forms an additive subgroup of the integers, since 
  \begin{itemize}
    \item the set of even numbers is non-empty (contains the additive identity, 0)
    \item every even number ${x}$ has additive inverse ${-x}$ which is also an even number for all even ${x}$
    \item the set is closed under addition, since the sum of 2 even numbers is always even
  \end{itemize}
  Now, for this group to be an ideal, it must satisfy the "absorption" property with respect to integer multiplication.\\
  However, consider the multiplication of integers:
  \begin{itemize}
    \item the product of 2 even integers is an even integer
    \item the product of an even and an odd integer is an even integer
  \end{itemize}
  So, the integer ring over integer multiplication and addition is an ideal.
\end{example}

The idea for ideals is similar to that of previous algebraic objects, however now it takes into account the idea of cosets.

All ideals are subrings in the ring in which they are considered.  Similarly to other sub-structures (subgroups, subrings, etc), ideals share the idea of proper and trivial.  In this case, a \textbf{proper ideal} is one which is a proper subring i.e. in which the set is a proper subset of the set considered in the ring.  If the ideal is an improper subring i.e. the set is the ring set itself, then this is labelled as a \textbf{unit ideal}.  

A simple extension to the idea of ideals is the concept of \textbf{principal ideals}.  This subset of ideals contains those which can be generated by a single element i.e. cyclic ideals.  Rings in which all ideals are principal are labelled \textbf{principal ideal rings}.  Note that if a ring is cyclic, then all ideals will be principal.  This is the case since in a cyclic ring, all subrings are cyclic.  Since ideals are subrings, it follows that all ideals in a cyclic ring are also cyclic.  Therefore, cyclic rings are principal ideal rings.  However, the converse does not hold i.e. the existence of a principal ideal does not mean that the ring itself is a principal ideal ring.  

Ideals can be specified over the subset of rings which are classified as more particular algebraic structures such as integral domains and fields.  In this case, the one of more interest is the \textbf{principal ideal domain}, which is an integral domain in which every ideal is principal.  

\begin{example}\label{ex: Principal ideal}
  
\end{example}

Another subtype of rings is a \textbf{Noetherian Ring}, introduced by Emmy Noether.  The idea of Noetherian rings relies on the \textbf{ascending chain condition} for ideals.

\begin{defn}\label{Ascending Chain Condition}
  The \textbf{ascending chain condition} is defined over a partially ordered set ${S}$ is satisfied if all increasing sequences over ${S}$ eventually converge to a constant.

  Note: not every sequence must converge to the same constant, but they all must converge.
\end{defn}

The ascending chain condition for ideals is defined in terms of ordering by the subset relation i.e. for any chain of ideals in some ring ${R}$, and ${k > 1}$
\begin{equation*}
  I_1 \subseteq \cdots \subseteq I_k \subseteq \cdots
\end{equation*}
there is an ${n \geq k}$ such that 
\begin{equation*}
  I_n = I_{n+1} = \cdots
\end{equation*}
i.e. for every ascending chain of ideals in ${R}$, the ascending chain condition is satisfied.

\begin{defn}\label{Noetherian rings}
  A \textbf{Noetherian ring} is a ring where the ascending chain condition is satisfied for sequences of ideals.
\end{defn}

\begin{note}
  Every finitely generated ring is Noetherian.

  \textbf{Proof:} 
\end{note}

Polynomial Ideals



\section{Monomial Ordering}

A \textbf{monomial} is a term consisting of the power product and the coefficient.  A polynomial consists of a sum of monomials.

Groebner bases are computed with respect to a particular monomial ordering; such an ordering must be defined for a given polynomial space.  For univariate polynomials, the ordering is fairly intuitive.

For polynomials ${p(x), q(x) \in}$ polynomial space ${D[x], \, p(x) = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0,}$ \\ ${\, a_i \in D\, q(x) = b_m x^m + b_{m-1} x^{m-1} + \ldots + b_1 x + b_0, \, b_i \in D}$, the ordering is defined as 
\begin{equation*}
  p(x) > q(x) \iff \, \textrm{either} \, n > m \, \textrm{or} \, n = m \; \textrm{and} \; \exists \, a_i > b_i \neq 0, \, \textrm{where} \, a_k = b_k \, \forall k > i 
\end{equation*}
This is a rather complex notation for the intuitive idea for univariate polynomial ordering.  In simpler terms, polynomial ${p > }$ polynomial ${q}$ if ${p}$ has a higher degree, or if they have the same degree, then if ${p}$ has higher values for one or more of its higher order coefficients.

\begin{example}\label{ex: Univariate ordering}
  Consider basic polynomials over the integers.
  \begin{equation*}
    2x^2 + 3x > 5x
  \end{equation*}
  Since the first polynomial is of higher degree.\\
  \begin{equation*}
    3x^2 + 2x > 2x^2 + 5x + 1
  \end{equation*}
  Since the polynomials are of equal degree, but ${3 > 2}$\\
  \begin{equation*}
    3x^2 + 2x > 3x^2 + 5
  \end{equation*}
  Since the polynomials are of equal degree but ${2x}$ is of higher degree than ${5}$
\end{example}

The idea of ordering can be extended to multivariate polynomials, however this requires slightly more work, as the concept is not as intuitive.  While ordering for univariate polynomials is strictly degree-based, when considering multivariate polynomials, the ordering must be considered both degree-wise and lexicographically.  The same polynomials can be ordered in various different ways depending on the monomial ordering chosen; this in turn can have large effects on the efficiency of algorithms working over this polynomial space, including the Groebner basis algorithms.

Whether degree-based or lexicographical (known henceforth as lex) ordering is selected, the variables must still be ordered with respect to each other i.e. an ordering must be chosen.  Then, in degree ordering the total degree is the first factor considered; so, monomials with a higher total degree are considered greater regardless of the degrees of the individual variables making up the monomial.  Then, if the total degree is the same, ordering is lexicographical given the order specified.  For lex, the ordering is the variable ordering specified.

To explain the differences and functionality of degree and lex ordering, demonstrative examples will be used.  

\begin{example}\label{ex: Multivariate degree ordering}
  Consider the polynomials ${x^2y^2, xy^3, x^3y, x^4y}$, with ordering ${x > y}$.\\
  The degree-based ordering would be as follows:
  ${x^4y >}$ everything else, since it has total degree 5 (sum of exponents) while the rest of the monomials have total degree 4.\\
  Then, ${x^3y > x^2y^2 > xy^3}$ since, when the total degrees are identical, then the ordering is based on the degree of the "greater" variable.

  Similarly, if the initial variable ordering chosen was ${y > x}$, the ordering would be as follows:
  ${x^4y >}$ everything else, since total degree is still 5.  Then, ${xy^3 > x^2y^2 > x^3y}$, since now ordering is with respect to y i.e. y is the "greater" variable.
\end{example}

In contrast, consider the same example but with lex ordering instead.
\begin{example}\label{ex: Multivariate lex ordering}
  Consider again the polynomials ${x^2y^2, xy^3, x^3y, x^4y}$, with ordering ${x > y}$.\\
  The lex ordering would be as follows:
  Then, ${x^4y > x^3y > x^2y^2 > xy^3}$ since the ordering is chosen with respect to the degree of ${x}$ alone (in the event of a tie in degree, then the ordering would be considered with respect to the degree of ${y}$).

  Similarly, if the initial variable ordering chosen was ${y > x}$, the ordering would be as follows:
  ${xy^3 > x^2y^2 >}$ the other terms, since degree in ${y}$ is the ordering factor (recall, the remaining terms have ${y^1}$).\\
  Then, for the remaining terms, ${x^4y > x^3y}$ since, as they have the same degree in ${y}$, ordering is now considered with respect to degree in ${x}$.
\end{example}

These ordering techniques can be applied to polynomials with an arbitrary number of variables.  Although not immediately clear from the concepts, the monomial ordering chosen (either degree-based or lex, and, within this choice, the ordering of the variables chosen) can have a large effect on the efficiency of the Groebner basis computations.  The same problem can be considered with different orderings and have massively different timings for the computations (as will be seen below with tested examples).


\section{Groebner Basis Characterization}

Polynomial ideals can be specified by a basis; this specifies a linearly independent generating set for the ideal.  However, this basis is not unique - there are many possible configurations or potential elements which could make up a basis for the ideal.  

A Groebner basis is a particular basis for a polynomial ideal, which satisfies various conditions, detailed below.  There are various advantages to this set.  The normalized Groebner basis is unique to each ideal, and can therefore be used as a sort of identifier for the ideal. 

There are various logically equivalent definitions for what makes a basis a Groebner basis.  One definition is as follows: 

\begin{defn}\label{Groebner Basis}
  G is a Groebner basis for ideal $I = <p_1, \ldots, p_n>$ over polynomial ring $F[x_1, \ldots, x_m]$ for a particular monomial ordering iff
  \begin{equation*}
    p \in I \iff Reduce( p, G) = 0
  \end{equation*}
\end{defn}

This definition, although concise, is not complete alone; the Reduce function has not been defined.

This concept of reduction is essential to Groebner bases, and to the computation of the bases using Buchberger's Algorithm.  Reduction can be defined of a polynomial over another polynomial, or a polynomial over a set of polynomials (such as in the definition above, ${Reduce( p, G)}$, reduction of polynomial ${p}$ over set ${G}$).  

Reducing polynomial ${p}$ over another polynomial ${q}$, ${Reduce( p, q)}$ amounts to multivariate division of ${\frac{p}{q}}$.  This is a recursive extension of the Euclidean algorithm for division of univariate polynomials, dependent on the monomial ordering specified.  The idea is to consider the multivariate polynomial as a "recursive univariate polynomial".  This concept is best described by an example: consider the polynomial ${}$ EXAMPLE ON THE SYMBOLIC MIDTERM FOR POLYNOMIAL LONG DIVISION.

A polynomial ${p}$ is considered \textbf{reducible} over polynomial ${q}$ if a monomial in ${p}$ can be written as a multiple of the leading monomial of ${q}$.  It is \textbf{irreducible} if there are no monomials in ${p}$ which are multiples of the leading monomial os ${q}$.    

\begin{note}
  The \textbf{leading monomial} of a polynomial is the monomial with the highest order with respect to the monomial ordering specified.
\end{note}   

Reduction over a set ${G}$ of polynomials is the iterative application of the ${Reduce}$ over each of the polynomials in the set, until it is irreducible over the whole set.  This reduced polynomial is termed the \textbf{normal form} of the polynomial with respect to the set.

Referring back to the definition of a Groebner basis above, the ${Reduce}$ can now be considered more precisely: a Groebner basis is a basis for a polynomial ideal ${I}$ over field ${F}$; polynomial ${p \in I}$ is a member of the basis if and only if it is irreducible over the basis.  This has the implication that every polynomial which is irreducible over the set is in the set; every other polynomial in the ideal has a leading monomial which is a multiple of one or more elements in the set.

\subsection{Buchberger's Algorithm}

The first algorithm for computing Groebner bases was proposed by Bruno Buchberger in 1965, when he presented the concept of the bases themselves.  This algorithm is fairly short in writing, but the complexity of the algorithm cannot be seen intuitively seen from the logic itself.  The algorithm is presented below in pseudocode.

\begin{algorithm}[H]
\caption{Buchberger}\label{buchberger}
\begin{algorithmic}[1]
\Input $\quad P$ generating set for a polynomial ideal; $ord$ monomial ordering
\Output Groebner Basis for the ideal generated by $P$
\Procedure{Buchberger}{}
\State $G \gets P$
\State $k \gets length( G)$
\State $B \gets \{[i, j] : 1 \leq i < j \leq k\}$\\
\While{ $B \neq \phi$}
  \State $[i, j] \gets selectpair( B, G)$
  \State $B \gets B - \{[i, j]\}$
  \State $h \gets Reduce( Spoly( G_i, G_j), G)$
  \If{ $h \neq 0$}
    \State $G \gets G \bigcup \{h\}; \qquad k \gets k + 1$
    \State $B \gets B \bigcup \{[i, k] : 1 \leq i < k\}$
  \EndIf
\EndWhile\\
\Return $G$
\EndProcedure
\end{algorithmic}
\end{algorithm}  

The basic idea is to start with a basis (i.e. a generating set) for the ideal, and repeatedly iterate over the set until all of the polynomials which are irreducible over the set are added to the set.  This is repeated until every combination of polynomials is either in the set or irreducible over the set.  Notice the call to ${Reduce}$; this is the reduction of a polynomial over the set ${G}$ as discussed above.   

One new concept which the algorithm requires is the computation of \textbf{S-polynomials}.  Computation of S-polynomials is done through the following formula: 

\begin{equation*}
  Spoly( p, q) = LCM(M(p), M(q)) \left( \frac{p}{M(p)} - \frac{q}{M(q)}\right)
\end{equation*}

\begin{note}
  A note on notation in the definition of S-polynomials, but which will be used elsewhere
  \begin{itemize}
    \item ${LCM}$ is the least common multiple
    \item ${M(p)}$ is the leading monomial of polynomial ${p}$, with the monomial ordering considered
  \end{itemize}
\end{note}

The S-polynomial is essentially an application of the reduction process; the result is the difference between the reduction of the LCM modulo ${p}$ and modulo ${q}$.  In the Buchberger algorithm, the S-polynomial is the tool by which it can be determined if the polynomial is reducible over the set and therefore whether or not to add it to the existing set.  To demonstrate the calculation process, an example of an S-polynomial derivation is included below.

\begin{example}\label{S-polynomial}
  Consider polynomials ${p, \, q \in }$ $\mathbb{Q}$, given by 
  \begin{equation*}
    p = 3x^2y - y^3 - 4 \qquad ; \qquad q = xy^3 + x^2 -9
  \end{equation*}
  under monomial ordering ${tdeg(x, y}$ (i.e. degree based ordering ${x > y}$)
  
  To find the S-polynomial ${Spoly( p, q)}$, first the leading monomials with respect to ${tdeg( x, y)}$ must be computed, in addition to their LCM.
  \begin{eqnarray*}
    M( p) &=& 3x^2y\\
    M( q) &=& xy^3\\\\
    LCM( M( p), M( q)) &=& 3x^2y^3
  \end{eqnarray*}
  
  So then, to calculate the S-polynomial
  \begin{eqnarray*}
    Spoly( p, q) &=& 3x^2y^3\left(\frac{3x^2y - y^3 - 4}{3x^2y} - \frac{xy^3 + x^2 -9}{xy^3}\right)\\
           &=& y^2(3x^2y - y^3 - 4) - 3x(xy^3 + x^2 -9)\\
           &=& -y^5 - 3x^3 - 4y^2 + 27x 
  \end{eqnarray*}
\end{example}

At this point all the necessary constructs for applying Buchberger's algorithm are present.  To demonstrate how this process is applied, an example walkthrough of the algorithm's execution is included for clarity.  Recall that as input, the algorithm requires a generating set for the polynomial ideal, and a monomial ordering specified.

EXAMPLE FROM NOTES

\subsection{F4 Algorithm}

Since the introduction of Groebner bases by Buchberger in 1965, there have been various improvements on his algorithm for basis computation.  Notably, there is the F4 algorithm, developed by Faugere.  There are F4 implementations in the Maple Groebner package (maplef4 algorithm, and efficient C implementation in the FGB algorithm).

This is a more efficient ..... 


Regardless of the improvements in speed of these algorithms, one problem which they all face is intermediate expression swell, as discussed above.  Groebner basis computations, depending on the initial ideal, can give rise to unmanagable expression swell.  This cannot be avoided by using classic iterative algorithms; however, these are the only algorithms currently available.  Instead of inventing an entirely new algorithmic approach, which may not even be possible, another potential approach would be to use the regular algorithms over prime fields, and then reconstruct the solution to the original space via a modular homomorphism method.

To discuss homomorphism methods, first the concept of a ring homomorphism must be introduced.

\begin{defn}\label{Homom}
  A mapping $\theta$ from ring ${R}$ to ring ${R'}$ is a ring homomorphism if
  \begin{itemize}
    \item ${\theta(a + b) = \theta(a) + \theta(b) \; \forall \, a, \, b \, \in R}$ 
    \item ${\theta(a * b) = \theta(a) * \theta(b) \; \forall \, a, \, b \, \in R}$ 
    \item ${\theta(1) = 1}$
  \end{itemize}

  Note that this also implies that ${\theta(0) = 0}$ and ${\theta(-a) = -\theta(a)}$ 
\end{defn}

Homomorphic mappings between rings preserve ring operations.  If a mapping satisfies the properties listed above (the homomorphism properties), then it is labelled a homomorphism.

Homomorphisms have many uses in algebra; in this particular case, the homomorphisms considered are a specific instance of ring homomorphisms: the \textbf{modular polynomial ring homomorphisms}.  A modular polynomial homomorphism is a mapping from a polynomial extension over the integers to a polynomial extension over the integers modulo a particular prime i.e. a modular field ${\mathbb{Z}_m}$ for a particular integer ${m}$.  Formally, this is defined as:

\begin{defn}\label{Modular Homom}
  A modular polynomial ring homomorphism is a homomorphism mapping
  \begin{equation*}\phi_m : \mathbb{Z}[x_1, \ldots, x_n] \rightarrow \mathbb{Z}_m[x_1, \ldots, x_n]\end{equation*}\\ where \\
  \begin{eqnarray*}
  \phi_m(x_i) = x_i; \; 1 \leq i \leq v; \quad and \\
  \phi_m(a) = a\mod m
  \end{eqnarray*}
  where ${a \mod m}$ is the remainder of ${\frac{a}{b}}$
\end{defn}

Modular homomorphisms can be used to simplify a problem.  For instance, for a situation in which large integers or rational numbers are required, the inputs could be projected to their homomorphic images in a simpler modular space, where the size of the integers is bound by the modular base i.e. ${m}$ for ${\mathbb{Z}_m}$.  This has various advantages over using the non-modular space:
\begin{itemize}
  \item The size of the values (coefficients in the extended ${\mathbb{Z}_m}$) have a known bound of ${m}$.  This means that the space necessary for each value can be specified in advance, which is an advantage when programming and using integers of a particular precision, to ensure that the values do not overflow the precision.
  \item The values will always be integers.\\The advantages of this are two-fold.  Firstly, this means that there are no rationals to contend with; this means less GCD computations, as there will be no need to reduce fractions to lowest terms post computation.  Not having fractions also simplified the computations in general, and reduces the amount of space required as only one value will be stored, rather than separate integers for the numerator and denominator.  Secondly, multiplicative inverses over the modular integers are also modular integers; this means that all computations involving "division" now consists of integer multiplication, again removing the GCD computations previously required.  It also means that the coefficient space of the polynomials is a field, so there are multiplicative inverses for all coefficients, unlike polynomials over the integers.
\end{itemize}

Working over a modular space seems like an optimal solution - this removes intermediate expression swell entirely, and the computational cost in both time and memory can be more accurately predicted (as the size of the values is known, and so the amount of memory necessary can be predicted, as can the time necessary to perform computations with these values).  However, there is a downside to this method: the final basis must be given in the space it originated in - either the rationals or the integers extended by a set of variables.  However, working with modular homomorphisms over a modular space means that the final basis returned from the computation will also be over this modular space.  

Working over a modular space means that the basis must be reconstructed back to the original space before the computation is complete.  There are various methods for reconstruction back from a modular space to an integer space; here the methods used are the Chinese Remainder Algorithm and Hensel's Algorithm.  Each of these has its particular points in regards to efficiency, and will be explained in detail below.  Post reconstruction, the basis will be represented in the integers extended by the polynomials; then, another reconstruction must be done to build the basis back to the initial space (the rational numbers extended by the polynomials).    

DIAGRAM WOULD BE NICE HERE showing the relation between initial space and final space, and the reconstruction algorithms

Before the modular reconstruction algorithms are discussed in detail, a new factor must be taken into account: there is a new problem associated with modular reconstructions.  Previously, working with the Groebner computation algorithms directly, there is no consideration for any external factors.  However, now the characteristics of a modular space must be considered.  In particular, the modular space must be a field; every element ${a}$ in the space (excluding the additive inverse, 0) must have a multiplicative inverse in the space i.e. a unique corresponding element ${a^{-1}}$ in the space such that ${a*a^{-1} = 1 = a^{-1}*a}$.  To ensure that this is the case, the positive integer ${m}$ in space ${\mathbb{Z}_m}$ must be a prime.

\begin{note}
  For the modular space ${\mathbb{Z}_m}$ to be a field, ${m}$ must be a prime.  One of the criteria for a space being a field is that there are no zero divisors i.e. 
  \begin{equation*}  \forall a, b \in \mathbb{Z}_m,\; a*b = 0 \, \textrm{means that either} \, a = 0 \, \textrm{or} \, b = 0 \end{equation*}
  However, if the modular space is over ${m}$ not a prime, then this means that ${m}$ must have factors other than the trivial factors 1 and ${m}$.  So then, this means that there are nontrivial ${a, b}$ such that ${m = a*b, \, a, b < m}$.   This implies that these ${a, b \in \mathbb{Z}_m}$, since ${a, b < m}$.  From this, it follows that 
  \begin{equation*} a*b = m = 0 \mod m \end{equation*}
  Since ${a, b}$ are both nonzero for ${m}$ nonzero, this means that there are zero divisors in ${\mathbb{Z}_m}$.  Therefore, ${\mathbb{Z}_m}$ is not a field for composite ${m}$.
\end{note}

\chapter{Modular Homomorphism Methods and Reconstruction}

Post processing in modular polynomial space requires reconstruction back to ${\mathbb{Z}[x_1, x_2, \ldots, x_n]}$, which is done through either the Chinese Remainder Algorithm (henceforth the CRA) or Hensel's Algorithm, which will be described below.  Subsequent reconstruction from the integers to the rationals is done using an algorithm similar to Farey rational reconstruction, which is also described below (WHICH PAPER).  This chapter also discusses a new difficulty which arises when attempting to reconstruct - the idea of \textbf{lucky primes}. 

\section{Chinese Remainder Theorem} 

The Chinese Remainder Algorithm (CRA) is a method for reconstruction of an integer (or set of integers); it requires a set of modular images over various primes, which, combined provide a mixed-radix representation of the integer being reconstructed.  

\subsection{Mixed Radix Representation}

In general, when numbers are considered as representations of a sum of values, this is in terms of a set of multiples of successive powers of a particular base.  The general definition is

\begin{defn}\label{Basic defn}
  For a value ${x}$ in base ${b}$,
  \begin{equation*}   x = \sum\limits_{i = 0}^{n}{a_ib^i}     \end{equation*}
  For ${\{a_i\}}$ a list of coefficients of ${x}$ also in base ${b}$
\end{defn}

This representation is \textbf{single-radix}; the radix being the base in which the number is being represented.  \textbf{Mixed-radix representation} is a similar idea, however instead of a linear combination of successive powers of a particular base, it is built from a linear combination of products of successive multiple "bases" - there is not a consistent base for all numerical positions.  The idea is similar in that the sequence of terms in the sum are in the base of a multiple of the previous base, however the factor of this multiple is not constant.  

One classic demonstrative example is in representing time: to tell the day, this is a sum of seconds and minutes and hours and days, each of which is a different successive radix.  There is no consistent base across all these terms, however each base is a multiple of the previous one (60 seconds in a minute, 60 minutes in an hour, 24 hours in a day, 7 days in a week).     

The CRA uses the various modular images provided to iteratively construct a mixed-radix representation of the original value, in order to perform the reconstruction.


\subsection{Chinese Remainder Algorithm} 

The Chinese Remainder Theorem (CRT) made its earliest appearance in the 3rd century, in the book \textit{Sunzi's Mathematical Classic}, by Sun Tzu, a Chinese mathematician.  The description of the solution to algorithmic solution to this problem was then described in the 6th century by another mathematician, Aryabhata.  Gauss also described the CRT, when he introduced the concept of modular congruences, in his 1801 book \textit{Disquisitiones Arithmeticae}.  (this is cited from elwiki, but I'm sure it's in another, more socially acceptable source)

The CRT has various representative definitions.  

\begin{theorem}\label{Chinese Remainder Theorem}
  
\end{theorem}

The CRA takes in as arguments a set of 

A demonstrative example of the CRA is included below, over the integers.  Note that the process is the same for polynomials; every coefficient in the polynomial is reconstructed via the CRA.

\begin{example}\label{CRA}
  Consider the following modular equivalences:
  \begin{eqnarray*}
    x &=& 2 \mod 3\\
    x &=& 3 \mod 7\\
    x &=& 0 \mod 2\\
    x &=& 0 \mod 5
  \end{eqnarray*}  
  2, 3, 5, 7 are all coprime; therefore, by the CRT, ${\exists}$ a unique solution$\mod$ 210 (where $\prod_{}^{}(2, 3, 5, 7) = 210$) 

  Now, calculate the components for the algorithm, following the algorithm as pseudocoded above.
  \begin{eqnarray*}
    M_2 = \frac{210}{2} = 105 \qquad ; \qquad \gamma_2 = (105)^{-1} \mod 2 = 1\\
    M_3 = \frac{210}{3} = 70 \qquad ; \qquad \gamma_3 = (70)^{-1} \mod 3 = 1\\
    M_5 = \frac{210}{5} = 42 \qquad ; \qquad \gamma_5 = (42)^{-1} \mod 5 = 3\\
    M_7 = \frac{210}{7} = 30 \qquad ; \qquad \gamma_7 = (30)^{-1} \mod 7 = 4
  \end{eqnarray*}

  Then, ${x}$ can be calculated as follows:
  \begin{eqnarray*}
    x &=& 0(M_2\gamma_2) + 2(M_3\gamma_3) + 0(M_5\gamma_5) + 3(M_7\gamma_7) \mod 210\\
      &=& 0 + 140 + 0 + 360 \mod 210\\
      &=& 500 \mod 210\\
      &=& 80
  \end{eqnarray*}

  So, the unique solution for ${x}$ as given by the modular images is 30.
\end{example}

\section{Hensel's Algorithm}

Hensel's algorithm, like 

\section{Rational Reconstruction}

The original basis in these algorithms is assumed to be the polynomial extended rational numbers, ${\mathbb{Q}[x_1, x_2, \ldots, x_n]}$.  As such, postprocessing must be done on the output of the CRA/Hensel reconstruction to build the basis back to the polynomials over the rationals.  

One common approach for rational reconstruction is using the Farey rational map.  The method as described by B\"ohm in his paper \textit{The Use of Bad Primes in Rational Reconstruction} is included below for explanation; this is also the method which was programmed in Maple as part of the modular methods.

The idea of rational reconstruction is to build back a number ${x \in \mathbb{Q}}$ from a modular image in ${\mathbb{Z}_p}$. 

The notation from the B\"ohm paper is used for clarity if reference is made to this paper.

Consider the rational number ${x \in \mathbb{Q}}$ which is being reconstructed.  Then, since ${x \in \mathbb{Q}}$ then ${x = \frac{a}{b}}$ for some ${a,\,b \in \mathbb{Z}}$ and some integer ${N \geq 2}$, with ${\gcd(a, b) = 1,\, \gcd(b, N) = 1}$ then
\begin{equation*}
  x_N = \left(\frac{a}{b}\right)_N = (a + N\mathbb{Z})(b + N\mathbb{Z})^{-1} \in \frac{\mathbb{Z}}{N\mathbb{Z}}
\end{equation*}   

Note that ${x_N}$ is a modular integer; as such, it is an equivalence class of rationals modulo ${N}$ i.e. the infinite set of rationals ${\frac{a}{b}}$ which are all equal modulo ${N}$.  This set is a member of the fraction field ${\frac{\mathbb{Z}}{N\mathbb{Z}}}$, which itself is isomorphic to ${\mathbb{Z}_N}$.

This approach to rational reconstruction uses the idea of a Farey rational map, which is defined (using B\"ohm's notation) for an integer ${B > 0}$ as:
\begin{equation*}
  F_B = \left\{\frac{a}{b} \in \mathbb{Z}\; : \; \gcd(a, b) = 1,\, 0 \leq a \leq B,\, 0 < |b| \leq B\right\}
\end{equation*}
Basically, the Farey rational map contains all the rational numbers (representatives of the equivalence classes of rational numbers i.e. fractions in lowest terms, where the GCD of the numerator and the denominator is 0) where the absolute value of the numerator and denominator range from 0 (1 for the denominator) to ${B}$.  This set will be finite for a finite ${B}$.

Then, define for ${m \in \frac{\mathbb{Z}}{N\mathbb{Z}}}$:
\begin{equation*}
  \mathbb{Q}_{N, m} = \left\{\frac{a}{b} \in \mathbb{Q}\; : \; \gcd(a, b) = 1,\, \gcd(b, N) = 1,\, \left(\frac{a}{b}\right)_N = m\right\}
\end{equation*}
the set of rational numbers which are congruent to ${m}$ modulo ${N}$.  

As B\"ohm states, the set ${\mathbb{Q}_N}$ can be expressed as the union of all the ${\mathbb{Q}_{N, m}}$ where ${m}$ ranges from 0 to ${N - 1}$.  This is fairly intuitive; each set is the equivalence class of ${m}$ modulo ${N}$ over the rational numbers; as such, the set of all possible ${m}$ ranges from 0 to ${N - 1}$ inclusive, as this is the range of the ${N}$-modular integers.  Therefore, there are ${N}$ distict equivalence classes, and the union of these sets is the entire set ${\mathbb{Q}_N}$ itself (as equivalence classes, by definition, form a partition of the set the relation is defined over; here, the relation is modulo, which is an equivalence relation).  

If ${B}$ is an integer where ${B \leq \sqrt(\frac{N - 1}{2})}$, then the \textbf{Farey map} can be defined as the mapping
\begin{equation*}
  \phi_{B, N}:\; F_B \bigcap \mathbb{Q}_N \rightarrow \frac{\mathbb{Z}}{N\mathbb{Z}},\quad \frac{a}{b} \rightarrow \left(\frac{a}{b}\right)_N
\end{equation*}

This mapping is well-defined and injective (as stated by B\"ohm).  This returns the original rational ${x}$.  To maximize image returned from this mapping with respect to ${N}$, choose ${B}$ to be the largest possible value for the ${N}$ specified.

The following algorithm calculates the Farey preimage as discussed in the B\"ohm paper; it returns the inverse image ${\phi_{B, N}^{-1}(y)}$ if ${y}$ is in the image of the Farey map with respect to the mapping ${\phi}$; if this value cannot be computed (for example, if the value attempting to be reconstructed does not fit the acceptance criteria, essentially amounting to the inverse image of modular integer ${N}$).

\begin{algorithm}[H]
\caption{Farey Reconstruction}\label{farey}
\begin{algorithmic}[1]
\Input $\quad$Integers $\; N \geq 2 \;$ and $\; 0 \leq r \leq N - 1$
\Output $\texttt{false} \,$or a rational number$\, a/b \,$ with$\, gcd(a, b) = 1,\, gcd(b, N) = 1,$ $\; a/b \equiv r $ mod $ N, \, 0 \leq a \leq \sqrt{(N-1)/2}, \, 0 \leq \abs{b} \leq \sqrt{(N-1)/2}.$ 
\Procedure{FareyRecon}{}
\State $a \gets [ N, r]$
\State $b \gets [ 0, 1]$
\State $i \gets 0$
\While{ $2a_{i+2}^2 \geq N-1$ } 
  \State $i \gets i + 1$
  \State $q \gets \text{floor}( a_i / a_{i+1})$
  \State $a_{i+2} \gets a_i - qa_{i+1}$
  \State $b_{i+2} \gets a_i - qb_{i+1}$
\EndWhile\\  
\If { $2b_{i+2}^2 < N-1$ \textbf{ and} $gcd( a_{i+2}, b_{i+2}) = 1$ } 
  \State \quad \Return $[ a_{i+2}, b_{i+2}]$
\Else 
  \State \quad \Return false
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Notice the algorithm returns the correct value for the rational reconstruction if it fits the acceptance criteria, otherwise it returns false in order to indicate to the user that there was an error in the computation; this would indicate that the value used to attempt the calculations was a "bad prime" with respect to rational reconstruction.

\section{Lucky Primes}

Naively, it would seem that using the smaller primes would make this process more efficient - smaller primes would mean a smaller upper bound on the coefficients, and therefore a more computationally efficient basis computation.  However, not every prime is valid for use in the reconstruction.  This can be seen directly in the rational reconstruction algorithm; some primes may return an invalid fraction if their magnitude is insufficient compared to the other inputs to the function.  

The CRA and Hensel's algorithm also have the potential for "bad" primes - this would be primes for which algebraic information is lost during processing in this modular space.  If during a computation there is a value for which this prime is a divisor, then this term would vanish when the basis is taken modulo the prime - then, this term can no longer be reconstructed, as it will no longer be present in any of later computations.

In Arnold's paper, she defines lucky primes as primes for which "we do not lose too much algebraic information" about ideal ${I \subseteq \mathbb{Q}[X]}$ when considering the ideal modulo prime p, ${I_p \subseteq \mathbb{Z}_p[X]}$.  For the modular lifting via CRA/Hensel, the required algebraic information is the set of leading terms of Groebner basis ${G}$ for ${I}$.

Formally (by Arnold):
\begin{defn}\label{Lucky Primes}
  A prime integer ${p}$ is a \textbf{lucky prime} for ideal ${I \subseteq \mathbb{Q}[X]}$ ${\iff}$ ${Lp(G) = Lp(G_p)}$
\end{defn}

\begin{note}
  A note on notation, with respect to the component parts of a polynomial.  Here, Arnold's notation will be adopted; note that in several other papers referenced the notation may not match precisely, and some translation is necessary to determine which of their notation is equivalent to which of Arnold's notation.  Here, Arnold's notation will be used throughout for consistency.  

  In this case, the Groebner basis is considered over Noetherian ring ${R = \mathbb{Q}[X]}$ 
  \begin{itemize}
    \item ${lp(f)}$ is the leading power product of polynomial ${f \in R}$
    \item ${lc(f)}$ is the leading coefficient of polynomial ${f \in R}$ (i.e. the coefficient of the leading power product)
    \item ${lt(f)}$ is the leading term of polynomial ${f \in R}$.  This is can be computed as ${lt(f) = lc(f)lp(f)}$ 
    \item ${Lp(f) = \{lp(f) \mid f \in R\}}$ is the set of leading power products of ${R}$
    \item ${Lc(f) = \{lc(f) \mid f \in R\}}$ is the set of leading coefficients of ${R}$
    \item ${Lt(f) = \{lt(f) \mid f \in R\}}$ is the set of leading terms of ${R}$
  \end{itemize}
\end{note}

Considering the definition of lucky primes as above, this relies on the Groebner basis having already been computed for ideal ${I}$.  This leads to some difficulties in using this definition to determine whether or not a prime is lucky for the actual computation - if the luckiness can only be determined post-processing with a comparison to the actual Groebner basis, this makes the luckiness checking fairly useless.  Instead, it would be convenient to be able to tell if a prime is lucky before performing the entire computation and checking if it results in a valid basis at the end.  

There are two alternate, more applicable definitions of lucky primes which will be considered in this case.

\subsection{Hilbert and Pauer}

The first definition to be considered is as given by Pauer in his paper on prime luckiness applied to Groebner bases.  

\begin{defn}\label{Pauer lucky}
  A prime ${p}$ is \textbf{Pauer lucky} for ideal ${I}$ if it does not divide any leading coefficient of any polynomial in ${G}$ the reduced Groebner basis of ${I}$.
\end{defn}

However, again, this relies on the true Groebner basis having been previously computed which essentially nullifies the algorithm.  


\chapter{Modular Groebner Basis Reconstruction}

The modular reconstruction methods introduced above can be applied to Groebner basis computations; they can be used in conjunction with the regular basis algorithms, over modular prime fields.  In the non-modular case, the algorithm (Buchberger, F4, or otherwise) can be applied directly to the supplied ideal basis, producing a single result which is the desired basis.  However, in the modular reconstruction case, there are added computations.  

For the CRA, various basis computations must be performed, each for the basis in a prime modular field, one for each of the primes necessary for the reconstruction.  The idea is that the various modular basis computations, coupled with the reconstruction code and the rational reconstruction will be more efficient (in both time and/or space) than the algorithm over the polynomial extended rational space, because of the elimination of the intermediate expression swell.  

For the CRA case, the algorithm proceeds iteratively:
\begin{enumerate}
  \item pick a good prime ${p_n}$
  \item perform a Groebner basis computation over ${\mathbb{Z}_{p_n}}$ coefficient field, returning ${G_{p_n}}$
  \item then, run the CRA reconstruction with ${G_{p_n}}$ and the basis ${G_{p_1p_2 \cdots p_{n-1}}}$, to build the basis ${G_{p_1p_2 \cdots p_{n-1}p_n}}$ 
  \item then, do the rational reconstruction on the new basis ${G_{p_1p_2 \cdots p_{n-1}p_n}}$ 
  \item the termination condition is if the rational reconstruction result for ${G_{p_1p_2 \cdots p_{n-1}p_n}}$ succeeds, and is identical to the reconstruction of ${G_{p_n}}$
  \item if the termination condition is not reached, then find another prime ${p_{n+1}}$ and repeat the process
\end{enumerate}

The algorithm selected for the Groebner basis computation does not change the process of the reconstruction; this is an external call.

Arnolds presented the idea of using modular homomorphism methods for eliminating intermediate expression swell.  PLEASE GO ON

\section{Maple}

To test the comparative efficiency of the modular reconstruction algorithms and the straight basis computation, the algorithms were implemented in Maple.  Therefore, the tests were performed relative to the built-in Maple Groebner package.  Similar tests could be performed in other symbolic languages testing versus their built-in Groebner functionality, such as Mathematica, Python's sympy, etc.  

Some technical details of the Maple Groebner basis package are discussed below:

There are various versions of the basis computation algorithms available in the Maple Groebner package; these include the classic Buchberger algorithm, the F4 algorithm implemented in Maple (maplef4), and the F4 algorithm implemented as an external C library, FGB.  In Maple, when a Groebner basis computation is performed, this amounts to a call to the Basis command in the Groebner package - the arguments specified are the basis for the current ideal, the monomial ordering used, and optional arguments specifying the algorithm type used, and the characteristic prime if working over a modular prime coefficient field.   

The current most efficient built-in basis algorithm in Maple is the FGB algorithm.  This is an efficient, optimized implementation of the Faugere F4 algorithm; it is written in C which also increases the efficiency of the algorithm.  However, this method suffers from lack of implementation of various options: for instance, it cannot be used for any monomial ordering other than degree-based (it does not support lex orderings).  In addition, when using the characteristic prime option to specify a modular field, FGB does not support primes greater than ${2^{16}}$, which becomes an issue when calling the modular reconstruction methods.  In these cases, various prime images are required, and this number increases as the primes are smaller.  In addition, smaller primes leads to the a higher probability of the primes being bad primes.  The comparative effect of efficiency between using more, smaller primes and fewer, larger primes will be examined in the results section.  However, note here that primes used for CRA/Hensel here were in the neighbourhood of ${2^32}$; in this way, the coefficients are guaranteed to be integers smaller than the maximum word space of the system, which means that the computations can be performed without external calls to (EXPLAIN THIS BETTER).  However, when using FGB the primes were restricted to the smaller size.

The more efficient algorithms also cause issues for Hensel construction; Hensel requires another optional argument in the basis computation, output=extended, which specifies that the basis computation should return some intermediate matrices (the Syzygy matrix and ONE OTHER) as discussed in the Pauer (???) paper.  The Hensel lifting algorithm itself is implemented with these matrices as input.  However, the functionality of Faugere's algorithm is such that the intermediate matrices are not computed, and therefore output=extended is not an option for the non-Buchberger algorithms.  

\chapter{Methodology and Results}

In order to obtain a wide range of data to have the potential to make conclusions, a set of problems must be constructed, representing the range of Groebner basis problems (a set of total degree, number of polynomials, coefficient size, integers/rationals, number of terms in the polynomial) was constructed and timing tests run on them with the various constructed algorithms, in order to compare.  These problems are included 

PSEUDOCODE OF THE MAPLE CALL

The results of the various algorithms running, with the various homomorphism implementations, were compared in terms of computational time.  

\noindent \textbf{Technical Details:}

The time computations were run on a VM server hosted at coscvm22.cs.laurentian.ca.  This VM was for the express purpose of these simulations, and as such there were no other user processes running on it.

\noindent \textbf{OS:} 64-bit Ubuntu 14.04\\
\textbf{Cores:} 6 logical cores\\
\textbf{Maple install:} 64-bit Maple 15

The timings were run with the Maple \texttt{time()} command.  This command returns the total CPU time used since the Maple session has started.  The time returned only includes the time for which maple was running, and not that during which the CPU was running other processes.  This is a more accurate measure of the time taken for the basis computation (as opposed to, for example, the bash \texttt{time} command).

Various problems were selected, in order to represent a range of each of the main factors affecting complexity of Groebner basis computations, as discussed above: degree, number of variables, and initial coefficient size.  There are also a range of problems in terms of the number of generating polynomials in the initial ideal, and the sparsity of these polynomials (i.e. the number of nonzero coefficients in the polynomials).  This range is qualitatively described in the table below.

TABLE HERE

The above problems were attempted with the various algorithms (Buchberger, MapleF4, and FGB) and methods (Hensel, CRA, and regular Maple).  The overall results are summarized in the following table; these are the overall Maple timings for the problems running on the server.  Details of how the running was implemented are included in Appendix \ref{appendix:running}.

\section{Raw Data}

-- here is the data


\refstepcounter{Table}
\begin{center}
  \begin{tabular}{| l || r | r | r || r | r || r | r ||}
  \multicolumn{8}{l}{\textbf{Table \theTable}: total timings for select problems}\\
  \hline 
  & \multicolumn{3}{c}{Buchberger} & \multicolumn{2}{c}{MapleF4} & \multicolumn{2}{c}{FGB} \TBstrut\\
  \hline
  Problems & CRA (s) & Hensel (s) & Maple (s) & CRA (s) & Maple (s) & CRA (s) & Maple (s)\\
  % Parameter & Average Relative Error & Variance of the Error \TBstrut \\ 
  \hline\hline
  arnborg1 & 0.723 & 18.969 & 0.423 & 0.532 & 0.312 & 0.1 & 0.036\\
  arnborg2 & 36.884 & \textbf{\textit{DNF}} & 14.812 & 9.661 & 3.673 & 0.56 & 0.072\\
  arnborg3 & 0.336 & 2340.014 & 0.2 & 0.264 & 0.139 & 0.159 & 0.036\\
  arnborg5 & 3.436 & \textbf{\textit{DNF}} & 1.408 & 3.436 & 1.072 & 0.367 & 0.052\\
  forsman1 & 0.192 & 0.891 & 0.084 & 0.239 & 0.136 & 0.063 & 0.028\\
  forsman2 & 0.028 & \textbf{\textit{DNF}} & 0.015 & 0.032 & 0.016 & 0.052 & 0.023\\
  forsman3 & 0.068 & \textbf{\textit{DNF}} & 0.108 & 0.068 & 0.028 & 0.059 & 0.024\\
  forsman4 & 0.804 & \textbf{\textit{DNF}} & 0.243 & 0.468 & 0.099 & 0.129 & 0.043\\
  katsura7 & 282.393 & \textbf{\textit{DNF}} & 24.604 & 153.189 & 38.533 & 16.554 & 0.496\\
  hell-in & 1.795 & 1.944 & 0.12 & 2.253 & 0.116 & 1.977 & 0.039\\
  kiyoshi5 & 1.128 & 1.212 & 0.016 & 1.152 & 0.024 & 2.321 & 0.044\\
  caprasse & 0.416 & 10.684 & 0.283 & 0.384 & 0.204 & 0.572 & 0.053\\
  mike2 & 38.936 & \textbf{\textit{DNF}} & 5.8 & 25.533 & 5.06 & 2.669 & 0.105\\
  p27 & 247.147 & \textbf{\textit{DNF}} & 34.236 & 66.784 & 11.92 & 4.88 & 0.283\\
  p33 & 0.443 & 1.176 & 0.035 & 0.4 & 0.08 & 0.519 & 0.04\\
  p34 & 1.72 & 1.924 & 0.079 & 2.04 & 0.132 & 1.9 & 0.051\\
  p41 & 20.956 & 89.82 & 0.456 & 27.987 & 1.08 & 16.964 & 0.264\\
  trav1 & 0.028 & 0.1 & 0.028 & 0.04 & 0.059 & 0.053 & 0.04\\
  cassou\_POSSO & 6.694 & \textbf{\textit{DNF}} & 2665.155 & 2.204 & 0.856 & 0.28 & 0.041\\
  cyclic6 & 37.444 & \textbf{\textit{DNF}} & 14.949 & 10.076 & 3.636 & 0.527 & 0.065\\
  cyclic7 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  cyclic8 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & 123.924\\
  hcyclic8 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  katsura8 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  katsura9 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  katsura10 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  repulso1 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  repulso2 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  repulso3 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}\\
  huygens3 & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}} & \textbf{\textit{DNF}}
  \\ % [1ex]
  \hline
  \end{tabular}
\end{center}

\begin{note}
  The cells marked \textit{\textbf{DNF}} indicate that this problem "did not finish"; in these cases it was due to the Maple kernel running out of memory and crashing with a "not enough memory available" or "object too large" error.  
\end{note}

The table above shows various expected trends.

\begin{itemize}
  \item Across all algorithms, the FGB is the fastest i.e. FGB Maple is faster than MapleF4 Maple and Buchberger Maple; and the same result can be seen for the CRA
  \item  Maple is faster than the CRA, which in turn is faster than Hensel, on a per-algorithm basis
\end{itemize}

These trends can also be visually represented by way of bar graphs.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{bb_tests.png}
  \caption{Buchberger's algorithm, comparing the methods \label{fig:bb_tests}}
\end{figure}

This graph represents the timings of the methods run with the modular computations done using Buchberger's algorithm, with 32-bit primes.  The \textbf{DNF} Hensel problems show up as no Hensel bar on the chart.  The Hensel problems are seen to be slower than the CRA and Maple in every case in which Hensel terminates.  Similarly, the CRA seems to be consistently slower than the nonmodular Maple Buchberger, although there are 2 problems in this selection (forsman3 and cassou\_POSSO) in which the CRA was faster.

Similar graphs can be made for the MapleF4 and FGB algorithms.  Note, however, that there is no Hensel data in either of these cases; this is because the Hensel method requires the Buchberger's output=extended option, as discussed above.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{f4_tests.png}
  \caption{MapleF4 algorithm, comparing the methods \label{fig:f4_tests}}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{fgb_tests.png}
  \caption{FGB algorithm, comparing the methods \label{fig:fgb_tests}}
\end{figure}

These graphs show the same trends as the Buchberger plot; the CRA is consistently slower than the nonmodular Maple in almost every case.  At this point, various conclusions could be drawn.

With regards to Hensel, this approach seems like a lost cause.  There are several points against it: for one, it seems to be slower in every case than the CRA and straight Maple approach in the Buchberger case.  In many cases, the algorithm did not terminate, while for the same problem it did terminate for other methods.  More importantly, Hensel can only be run with the modular basis computations using Buchberger's algorithm; since it relies on output=extended, it cannot make use of the faster basis computation algorithms available.  %lolol

However, although the CRA is still consistently slower than Maple, there are still potential efficiency improvements to the CRA.

\section{Potential Improvements to the CRA} 

The CRA computation can be broken down into stages:

\begin{enumerate}
  \item Modular basis computations, to generate modular images for reconstruction
  \item CRA reconstruction itself
  \item Farey reconstruction
\end{enumerate}

The modular basis computations are done using the Maple Groebner package, with whichever algorithm was selected; if this is the stage with the least time efficiency, then there is no clear approach to solving the problem.  This would mean that the Groebner package itself is not well-suited for the CRA, and fixing this would involve editing the Maple package itself, which cannot be done as a user as they do not provide the source code for their compiled libraries.

However, if the primary time cost is with either the CRA or Farey reconstruction, then this problem could potentially be addressed by other means.  Maple provides user the option of writing their own external libraries (in either C, Java, or FORTRAN).  In this case, approach would be to use C and the GMP MPZ library.  This is the GNU MultiPrecision package; the C MPZ library allows for manipulation of multiprecision integers as objects in C.  Using this, the CRA could be written in C and linked as an external library to Maple, in order to speed up the computation.  C is naturally faster than Maple, and allows for optimization using multithreading and other lower level functionality which is not available in Maple.  In addition, although the current code is using the Maple \texttt{iratrecon} reconstruction method, the Farey reconstruction could also be written in C and linked in similar to the CRA reconstruction.

The technical details of passing arguments and linking the library are included in \textbf{Appendix} \ref{appendix:running}.  

Profiling of the CRA runs was done to compare the proportion of total time during each stage of the CRA basis computations.  The results are included for all 3 algorithms in the tables below.  

The column headers are:
\begin{itemize}
  \item Mod Basis: time spent performing modular basis computations with the Groebner package (in seconds)
  \item Farey: time spent running rational reconstructions (in seconds)
  \item CRA: time spent running CRA reconstructions (in seconds)
  \item Total: total time for the CRA basis computation (sum of the above 3 terms) (in seconds)
  \item Images: the number of modular images which were required
\end{itemize} 

\refstepcounter{Table}
\refstepcounter{Table}
\begin{center}
  \begin{tabular}{| l || r | r | r || r || r ||}
  \multicolumn{6}{l}{\textbf{Table \theTable}: CRA profiling, Buchberger's algorithm (32-bit primes)}\TBstrut\\
  \hline 
  Problems & Mod Basis (s) & Farey (s) & CRA (s) & \textbf{Total} (s) & Images \\
  % Parameter & Average Relative Error & Variance of the Error \TBstrut \\ 
  \hline\hline
   arnborg1 & 0.715 & 0.008 & 0 & 0.723 & 2 \\
   arnborg2 & 36.62 & 0.24 & 0.024 & 36.884 & 5 \\
   arnborg3 & 0.308 & 0.028 & 0 & 0.336 & 2 \\
   arnborg5 & 5.201 & 0.156 & 0.007 & 5.364 & 4 \\
   forsman1 & 0.168 & 0.02 & 0.004 & 0.192 & 2 \\
   forsman2 & 0.024 & 0.004 & 0 & 0.028 & 2 \\
   forsman3 & 0.204 & 0 & 0.012 & 0.216 & 2 \\
   forsman4 & 0.792 & 0.012 & 0 & 0.804 & 5 \\
   katsura7 & 272.61 & 9.407 & 0.376 & 282.393 & 13 \\
   hell-in & 1.041 & 0.726 & 0.028 & 1.795 & 21 \\
   kiyoshi5 & 0.244 & 0.872 & 0.012 & 1.128 & 71 \\
   caprasse & 0.395 & 0.017 & 0.004 & 0.416 & 2 \\
   mike2 & 37.605 & 1.104 & 0.227 & 38.936 & 8 \\
   p27 & 245.539 & 1.515 & 0.093 & 247.147 & 8 \\
   p33 & 0.221 & 0.21 & 0.012 & 0.443 & 17 \\
   p34 & 0.885 & 0.815 & 0.02 & 1.72 & 21 \\
   p41 & 12.214 & 8.511 & 0.231 & 20.956 & 41 \\
   trav1 & 0.028 & 0 & 0 & 0.028 & 2 \\
   cassou\_POSSO & 6.622 & 0.068 & 0.004 & 6.694 & 11 \\
   cyclic6 & 37.187 & 0.233 & 0.024 & 37.444 & 5
  \\ % [1ex]
  \hline
  \end{tabular}
\end{center}

\refstepcounter{Table}
\begin{center}
  \begin{tabular}{| l || r | r | r || r || r ||}
  \multicolumn{6}{l}{\textbf{Table \theTable}: CRA profiling, MapleF4 algorithm (32-bit primes)}\TBstrut\\
  \hline 
  Problems & Mod Basis (s) & Farey (s) & CRA (s) & \textbf{Total} (s) & Images \\
  % Parameter & Average Relative Error & Variance of the Error \TBstrut \\ 
  \hline\hline
   arnborg1 & 0.52 & 0.012 & 0 & 0.532 & 2 \\
   arnborg2 & 9.376 & 0.279 & 0.006 & 9.661 & 5 \\
   arnborg3 & 0.24 & 0.024 & 0 & 0.264 & 2 \\
   arnborg5 & 3.252 & 0.18 & 0.004 & 3.436 & 4 \\
   forsman1 & 0.226 & 0.013 & 0 & 0.239 & 2 \\
   forsman2 & 0.028 & 0.004 & 0 & 0.032 & 2 \\
   forsman3 & 0.068 & 0 & 0 & 0.068 & 2 \\
   forsman4 & 0.456 & 0.008 & 0.004 & 0.468 & 5 \\
   katsura7 & 143.325 & 9.604 & 0.26 & 153.189 & 13 \\
   hell-in & 1.312 & 0.929 & 0.012 & 2.253 & 21 \\
   kiyoshi5 & 0.37 & 0.766 & 0.016 & 1.152 & 71 \\
   caprasse & 0.368 & 0.016 & 0 & 0.384 & 2 \\
   mike2 & 24.343 & 1.129 & 0.061 & 25.533 & 8 \\
   p27 & 65.269 & 1.444 & 0.071 & 66.784 & 8 \\
   p33 & 0.234 & 0.146 & 0.02 & 0.4 & 17 \\
   p34 & 1.219 & 0.793 & 0.028 & 2.04 & 21 \\
   p41 & 19.183 & 8.666 & 0.138 & 27.987 & 41 \\
   trav1 & 0.036 & 0.004 & 0 & 0.04 & 2 \\
   cassou\_POSSO & 2.144 & 0.056 & 0.004 & 2.204 & 11 \\
   cyclic6 & 9.533 & 0.428 & 0.115 & 10.076 & 5
  \\ % [1ex]
  \hline
  \end{tabular}
\end{center}

\refstepcounter{Table}
\begin{center}
  \begin{tabular}{| l || r | r | r || r || r ||}
  \multicolumn{6}{l}{\textbf{Table \theTable}: CRA profiling, FGB algorithm (16-bit primes)}\TBstrut\\
  \hline 
  Problems & Mod Basis (s) & Farey (s) & CRA (s) & \textbf{Total} (s) & Images \\
  % Parameter & Average Relative Error & Variance of the Error \TBstrut \\ 
  \hline\hline
  arnborg1 & 0.063 & 0.033 & 0.004 & 0.1 & 4\\
  arnborg2 & 0.259 & 0.277 & 0.024 & 0.56 & 7\\
  arnborg3 & 0.08 & 0.071 & 0.008 & 0.159 & 4\\
  arnborg5 & 0.142 & 0.186 & 0.039 & 0.367 & 6\\
  forsman1 & 0.055 & 0.008 & 0 & 0.063 & 2\\
  forsman2 & 0.048 & 0.004 & 0 & 0.052 & 4\\
  forsman3 & 0.055 & 0.004 & 0 & 0.059 & 4\\
  forsman4 & 0.109 & 0.016 & 0.004 & 0.129 & 8\\
  katsura7 & 2.39 & 13.629 & 0.535 & 16.554 & 23\\
  hell-in & 0.317 & 1.585 & 0.075 & 1.977 & 38\\
  kiyoshi5 & 0.689 & 1.596 & 0.036 & 2.321 & 135\\
  caprasse & 0.137 & 0.04 & 0.395 & 0.572 & 4\\
  mike2 & 0.564 & 1.953 & 0.152 & 2.669 & 13\\
  p27 & 1.974 & 2.736 & 0.17 & 4.88 & 12\\
  p33 & 0.195 & 0.251 & 0.073 & 0.519 & 31\\
  p34 & 0.289 & 1.51 & 0.101 & 1.9 & 38\\
  p41 & 1.613 & 14.938 & 0.413 & 16.964 & 77\\
  trav1 & 0.048 & 0.005 & 0 & 0.053 & 2\\
  cassou\_POSSO & 0.169 & 0.075 & 0.036 & 0.28 & 19\\
  cyclic6 & 0.235 & 0.271 & 0.021 & 0.527 & 7
  \\ % [1ex]
  \hline
  \end{tabular}
\end{center}

Examination of the above tables yields the unfortunate conclusion that the majority of the time spent during the CRA basis computations is spent doing the modular basis computations with the Groebner package; this is consistent across all 3 algorithms.  Since the Groebner package is unmodifiable by the user, this means that this section of the CRA process cannot be optimized.  Graphs were made comparing the total modular basis computation time (i.e. the section of CRA basis computation time spent on modular images alone) with the nonmodular Maple computation.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{bb_mapleVsCRA_modBasisTime.png}
  \caption{Buchberger's algorithm, comparing nonmodular to CRA mod basis time \label{fig:f4_tests}}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{f4_mapleVsCRA_modBasisTime.png}
  \caption{MapleF4 algorithm, comparing nonmodular to CRA mod basis time \label{fig:f4_tests}}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{fgb_mapleVsCRA_modBasisTime.png}
  \caption{FGB algorithm, comparing nonmodular to CRA mod basis time \label{fig:f4_tests}}
\end{figure}  

The above plots show that even with the modular basis computation time alone, the CRA basis method is already slower than the nonmodular basis computation.  Therefore, any added efficiency in terms of the Farey and CRA reconstruction methods using C or another optimization method would effectively be irrelevant, as the method cannot be any faster than the modular basis time alone, and this cannot be changed by the user.  

One potential alteration to the method would be to use different-sized primes; 16-bit primes were also used, however this was found to be slower (data included in \textbf{Appendix} \ref{appendix:supplData}).  

Going the other way and using bigger primes could be another approach, however this is also flawed.  The 64-bit Maple has a word-size of 64 bits; using primes greater than 32-bit would mean that their product would no longer be word-size, and therefore the computations would be less efficient.  In additon, increasing the prime size would increase the intermediate expression swell.  Consider that in the limit of the primes reaching the maximum size of the final basis coefficients, then this modular computation would be exactly equivalent to the nonmodular method, and both methods should be equal in necessary time.  Removing the efficiency of using word-size primes for the computations means that the modular computations will not be faster than the nonmodular methods; at fastest, they would be equal to the nonmodular methods.  

So in conclusion, it seems that the current Maple Groebner package makes the attempted modular homomorphism methods redundant.  No added efficiency was observed; in fact, the modular approach was consistently slower than the basic nonmodular approach, for all available algorithms.  

However, this does not mean that the modular approach to Groebner basis computation is a dead end. 

\section{Future Research}    

Although in this case (considering Maple and the current Groebner package) the modular homomorphism approach turned out to decrease the time efficiency of the basis computations, there are various avenues to potentially explore in future research.  

In terms of Maple, the main roadblock is the inefficiency of the current modular basis computations.  Potentially, if a new release of Maple were to include a more efficient algorithm for the modular basis computation, this could tip the scales in favour of the modular approach.  Considering improvements to the existing Maple code, if the FGB code could include the capability of working over 32-bit primes this would increase the efficiency of the modular methods building the images using this algorithm.  The 32-bit primes were seen to be more efficient than the 16-bit primes for the other 2 algorithms, so it seems a logical conclusion that 32-bit prime processing would be faster in FGB as well.  

There are also computer algebra systems other than Maple.  Mathematica, Singular, and Python's sympy package all have Groebner plugins available; the experiment could be repeated using other available software packages.  EXPLAIN A BIT ABOUT THESE; MAYBE PYTHON HAS OPENSOURCE GROEBNER SHIT

\begin{appendices}

\chapter{Supplementary Definitions}\label{appendix:definitions}

This contains definitions which are presumed known and not explicitly declared in the text, but are referred to in passing.  They are included here for clarity.

\begin{defn}\label{defn: Unit}
  An element ${u \in D, \, D}$ domain, is a \textbf{unit} (\textbf{invertible}) if ${u}$ has a multiplicative inverse (i.e. with respect to operation ${*\,}$in ${D}$).

  For example, in $\mathbb{Z}$, the units are 1 and -1.
\end{defn}

\begin{defn}\label{defn: Associate}
  Two elements ${a, b \in D, \, D}$ domain, are \textbf{associates} if ${a \mid b}$ and ${b \mid a}$ i.e. they are multiples they are related via multiplication of a unit. 
\end{defn}

\begin{defn}\label{defn: Prime}
  An element ${p \in D - \{0\}, \, D}$ domain, is a \textbf{prime} if
  \begin{itemize}
    \item ${p}$ is \textbf{not} a unit
    \item whenever ${p = a \cdot b}$, for ${a, b \in D}$, then either ${a}$ or ${b}$ is a unit (but not both, since ${p}$ is not a unit)
  \end{itemize}
\end{defn}


\chapter{Implementation Details}\label{appendix:running}

\section{Batch Running}

This section gives a brief description of how the problems were timed.

The examples are included each in their individual Maple files; there is another Maple file which reads them all in and stores their initial ideal basis, problem name, and monomial ordering in parallel arrays.  The initial approach was to have one Maple file, \texttt{timer.mpl}, which ran each method and algorithm on each of the problems sequentially, printing the output of each to a named file before proceeding to the next problem.  This simple, Maple-only approach looping through all the problems was the naive approach, however it ran into issues with Maple's garbage collection: by the time the second problem was being computed, the memory from the first problem had not yet been released and was still taking up space in the Maple VM.  This caused two issues.
\begin{itemize}
  \item If the order of problem computations was reversed (i.e. if problems ran in order A then B, and then B then A), then the timings were significantly different; the second problem to execute always took much more time than if the same problem was executed first.  This is due to the additional memory which is taken up by the time the second problem begins executing; all the memory from the first is still taken up, and the extra garbage collections necessary to make room for the second computation adds additional time to the run.
  \item Another problem is the garbage collector itself; it seems to not deallocate memory even when the variable associated is reassigned or removed.  This leads to issues when multiple problems are computed sequentially in the same Maple session.  The Maple VM has a limited amount of memory on the server, and, even if this memory limit is increased, it is still a finite value.  The Groebner basis problems use so much memory that this limit is quickly reached, particularly when some of the Maple memory is already taken up from the previous problem.  When the memory limit is reached, the Maple session crashes with an error; this means that, if all the problems were computed in the same session, then the later problems would never be reached for computation as the kernel would run out of memory first.  
\end{itemize} 
The solution was to have one \texttt{timer.mpl} which computes one particular, specified problem, and a bash script to loop over all the problems and specify to the timer file which problem to run, and which combination of algorithm and method to use.

\begin{note}
  Maple does not provide the functionality of command line arguments as most programming languages provide.  For example, one cannot execute a Maple program by calling \begin{equation*}\texttt{maple mapleProgram arg1 arg2 ...}\end{equation*} specifying \texttt{arg1 arg2 ...} as command line arguments to the program.

  To get around this, the bash sed utility was used to modify the flag variables to specify which problem, algorithm, and method to run the timer with.
\end{note}

Then, all the problems are executed in their own Maple sessions to avoid any memory inconsistencies.

\section{Maple External C}


\chapter{Supplementary Data}\label{appendix:supplData}

The CRA was run using Buchberger and MapleF4 with 16-bit primes in addition to the 32-bit prime data included in the body of the thesis.  This was mainly done in order to have a more "level" comparison with the FGB CRA, which can only use 16-bit primes, as the option for larger primes has not been included in the CRA.  It was found that the 16-bit primes were slower than the 32-bit primes for Buchberger and F4; this seems to be due to the larger number of modular images which are necessary for the smaller primes, and that the majority of the time spent on CRA Groebner computations is in the calls to the modular basis computations in the Groebner package.

The FGB 16-bit prime data is included above in the \textbf{Methodology and Results} chapter.  

Note that, as above, the \textbf{DNF} problems are not included in the CRA data tables.

\refstepcounter{Table}
\begin{center}
  \begin{tabular}{| l || r | r | r || r || r ||}
  \multicolumn{6}{l}{\textbf{Table \theTable}: CRA profiling, Buchberger algorithm (16-bit primes)}\TBstrut\\
  \hline 
  Problems & Mod Basis (s) & Farey (s) & CRA (s) & \textbf{Total} (s) & Images \\
  % Parameter & Average Relative Error & Variance of the Error \TBstrut \\ 
  \hline\hline
  arnborg1 & 1.093 & 0.023 & 0 & 1.116 & 4\\
  arnborg2 & 49.568 & 0.272 & 0.024 & 49.864 & 7\\
  arnborg3 & 0.582 & 0.024 & 0.007 & 0.613 & 4\\
  arnborg5 & 6.868 & 0.188 & 0.012 & 7.068 & 6\\
  forsman1 & 0.144 & 0.008 & 0 & 0.152 & 2\\
  forsman2 & 0.041 & 0.004 & 0 & 0.045 & 4\\
  forsman3 & 0.332 & 0.016 & 0 & 0.348 & 4\\
  forsman4 & 0.981 & 0.016 & 0.008 & 1.005 & 8\\
  katsura7 & 399.268 & 14.618 & 0.442 & 414.328 & 23\\
  hell-in & 1.467 & 1.464 & 0.04 & 2.971 & 38\\
  kiyoshi5 & 0.534 & 1.461 & 0.041 & 2.036 & 135\\
  caprasse & 0.78 & 0.044 & 0 & 0.824 & 4\\
  mike2 & 54.882 & 1.807 & 0.1 & 56.789 & 13\\
  p27 & 377.798 & 2.331 & 0.117 & 380.246 & 12\\
  p33 & 0.302 & 0.297 & 0.017 & 0.616 & 31\\
  p34 & 1.426 & 1.453 & 0.045 & 2.924 & 38\\
  p41 & 20.023 & 14.799 & 0.553 & 35.375 & 77\\
  trav1 & 0.032 & 0 & 0 & 0.032 & 2\\
  cassou\_POSSO & 9.836 & 0.06 & 0.004 & 9.9 & 19\\
  cyclic6 & 50.378 & 0.279 & 0.031 & 50.688 & 7
  \\ % [1ex]
  \hline
  \end{tabular}
\end{center}

\refstepcounter{Table}
\begin{center}
  \begin{tabular}{| l || r | r | r || r || r ||}
  \multicolumn{6}{l}{\textbf{Table \theTable}: CRA profiling, MapleF4 algorithm (16-bit primes)}\TBstrut\\
  \hline 
  Problems & Mod Basis (s) & Farey (s) & CRA (s) & \textbf{Total} (s) & Images \\
  % Parameter & Average Relative Error & Variance of the Error \TBstrut \\ 
  \hline\hline
  arnborg1 & 0.816 & 0.036 & 0 & 0.852 & 4\\
  arnborg2 & 11.825 & 0.274 & 0.012 & 12.111 & 7\\
  arnborg3 & 0.437 & 0.028 & 0 & 0.465 & 4\\
  arnborg5 & 4.2 & 0.168 & 0.013 & 4.381 & 6\\
  forsman1 & 0.222 & 0.013 & 0 & 0.235 & 2\\
  forsman2 & 0.043 & 0.012 & 0 & 0.055 & 4\\
  forsman3 & 0.107 & 0.009 & 0 & 0.116 & 4\\
  forsman4 & 0.644 & 0.012 & 0 & 0.656 & 8\\
  katsura7 & 182.875 & 13.931 & 0.662 & 197.468 & 23\\
  hell-in & 2.08 & 1.517 & 0.044 & 3.641 & 38\\
  kiyoshi5 & 0.677 & 1.487 & 0.036 & 2.2 & 135\\
  caprasse & 0.684 & 0.037 & 0 & 0.721 & 4\\
  mike2 & 29.621 & 1.707 & 0.247 & 31.575 & 13\\
  p27 & 93.244 & 3.572 & 0.127 & 96.943 & 12\\
  p33 & 0.371 & 0.265 & 0.004 & 0.64 & 31\\
  p34 & 2.093 & 1.536 & 0.048 & 3.677 & 38\\
  p41 & 31.701 & 15.124 & 0.456 & 47.281 & 77\\
  trav1 & 0.036 & 0.004 & 0 & 0.04 & 2\\
  cassou\_POSSO & 3.381 & 0.08 & 0.016 & 3.477 & 19\\
  cyclic6 & 11.676 & 0.283 & 0.021 & 11.98 & 7
  \\ % [1ex]
  \hline
  \end{tabular}
\end{center}

Referring back to the 32-bit data tables for the CRA with these algorithms respectively, it is noted that the number of modular images here is the same as those necessary for the 16-bit FGB CRA, and close to twice that of the 32-bit CRA.  This is expected, as less prime images are necessary as the size of the prime increases; less primes are required for the product to be larger enough for the reconstruction to stabilize at the unique solution.

\end{appendices}






%----------------------------------------------------------------------
% END MATERIAL
%----------------------------------------------------------------------

% B I B L I O G R A P H Y
% -----------------------

% The following statement selects the style to use for references.  It controls the sort order of the entries in the bibliography and also the formatting for the in-text labels.
\bibliographystyle{plain}
% This specifies the location of the file containing the bibliographic information.  
% It assumes you're using BibTeX (if not, why not?).
\cleardoublepage % This is needed if the book class is used, to place the anchor in the correct page,
                 % because the bibliography will start on its own page.
                 % Use \clearpage instead if the document class uses the "oneside" argument
\phantomsection  % With hyperref package, enables hyperlinking from the table of contents to bibliography             
% The following statement causes the title "References" to be used for the bibliography section:
\renewcommand*{\bibname}{References}

% Add the References to the Table of Contents
\addcontentsline{toc}{chapter}{\textbf{References}}

\bibliography{uw-ethesis}
% Tip 5: You can create multiple .bib files to organize your references. 
% Just list them all in the \bibliogaphy command, separated by commas (no spaces).

% The following statement causes the specified references to be added to the bibliography% even if they were not 
% cited in the text. The asterisk is a wildcard that causes all entries in the bibliographic database to be included (optional).
\nocite{*}

\end{document}